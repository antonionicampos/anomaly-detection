{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "pressed-dining",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from dataset import Dataset\n",
    "from datetime import datetime\n",
    "from model import Autoencoder\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "harmful-latitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, epochs=20):\n",
    "    \n",
    "    print('starting model training...')\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"training on {'cuda' if torch.cuda.is_available() else 'cpu'}\")\n",
    "    model.to(device)\n",
    "    history = []\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        start = time.time()\n",
    "        model.train()\n",
    "        training_loss, running_loss = 0.0, 0.0\n",
    "        \n",
    "        # Iterando sobre o dataset\n",
    "        for batch_i, data in enumerate(train_loader):\n",
    "            X = data['X'].to(device)\n",
    "            \n",
    "            # Zero Grad\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward Pass\n",
    "            output = model(X)\n",
    "\n",
    "            # Loss Function\n",
    "            loss = criterion(output, X)\n",
    "\n",
    "            # Backward Pass\n",
    "            loss.backward()\n",
    "            running_loss += loss.item()\n",
    "            training_loss += loss.item()\n",
    "\n",
    "            # Update\n",
    "            optimizer.step()\n",
    "\n",
    "            if batch_i % 100 == 99:\n",
    "                print('Batch: {}, Avg. Loss: {}'.format(batch_i + 1, running_loss / 100))\n",
    "                running_loss = 0.0\n",
    "        \n",
    "        # Epoch results\n",
    "        training_loss /= len(train_loader)\n",
    "        print(f'[{round(time.time() - start, 3)} secs] Epoch: {epoch+1}/{epochs}', end='')\n",
    "        print(f', Training loss: {training_loss}', end='\\n\\n')\n",
    "        history.append(training_loss)\n",
    "    \n",
    "    print('training finished.')\n",
    "    \n",
    "    date = str(datetime.now()).split('.')[0]    \n",
    "    model_path = f'.\\\\models\\\\model_{re.sub(r\"[^0-9]\", \"\", date)}.pt'\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': training_loss\n",
    "    }, model_path)\n",
    "    \n",
    "    print('model saved.')\n",
    "    \n",
    "    return history\n",
    "\n",
    "def load_model(path):\n",
    "    \n",
    "    model = Autoencoder(input_dim=dim)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    epoch = checkpoint['epoch']\n",
    "    train_loss = checkpoint['loss']\n",
    "    \n",
    "    return model, optimizer, epoch, train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "forced-emission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (encoder1): Linear(in_features=52, out_features=256, bias=True)\n",
      "  (encoder2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (encoder3): Linear(in_features=128, out_features=26, bias=True)\n",
      "  (decoder1): Linear(in_features=26, out_features=128, bias=True)\n",
      "  (decoder2): Linear(in_features=128, out_features=256, bias=True)\n",
      "  (decoder3): Linear(in_features=256, out_features=52, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "train_dataset = Dataset(path='.\\\\data\\\\training.h5', key='normal')\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=0)\n",
    "\n",
    "dim = train_dataset[0]['X'].shape[0]\n",
    "\n",
    "model = Autoencoder(input_dim=dim)\n",
    "print(model)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accepted-benchmark",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting model training...\n",
      "training on cuda\n",
      "Batch: 100, Avg. Loss: 0.6757068499922753\n",
      "Batch: 200, Avg. Loss: 0.39336280435323717\n",
      "Batch: 300, Avg. Loss: 0.311646970808506\n",
      "Batch: 400, Avg. Loss: 0.2943175122141838\n",
      "Batch: 500, Avg. Loss: 0.2919827476143837\n",
      "Batch: 600, Avg. Loss: 0.28560491412878036\n",
      "Batch: 700, Avg. Loss: 0.27827429831027983\n",
      "Batch: 800, Avg. Loss: 0.2761135169863701\n",
      "Batch: 900, Avg. Loss: 0.2751822239160538\n",
      "Batch: 1000, Avg. Loss: 0.2722132921218872\n",
      "Batch: 1100, Avg. Loss: 0.2693168011307716\n",
      "Batch: 1200, Avg. Loss: 0.269123744815588\n",
      "Batch: 1300, Avg. Loss: 0.26979887127876284\n",
      "Batch: 1400, Avg. Loss: 0.27036403834819794\n",
      "Batch: 1500, Avg. Loss: 0.2696216785907745\n",
      "Batch: 1600, Avg. Loss: 0.26904914289712906\n",
      "Batch: 1700, Avg. Loss: 0.26893986463546754\n",
      "Batch: 1800, Avg. Loss: 0.2681207522749901\n",
      "Batch: 1900, Avg. Loss: 0.2668739515542984\n",
      "[30.443 secs] Epoch: 1/50, Training loss: 0.30296513886632304\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.2670369490981102\n",
      "Batch: 200, Avg. Loss: 0.2674601525068283\n",
      "Batch: 300, Avg. Loss: 0.26502151921391487\n",
      "Batch: 400, Avg. Loss: 0.2670530092716217\n",
      "Batch: 500, Avg. Loss: 0.26549599051475525\n",
      "Batch: 600, Avg. Loss: 0.26675028577446935\n",
      "Batch: 700, Avg. Loss: 0.26560048639774325\n",
      "Batch: 800, Avg. Loss: 0.26491319343447683\n",
      "Batch: 900, Avg. Loss: 0.26512806355953217\n",
      "Batch: 1000, Avg. Loss: 0.26456598430871964\n",
      "Batch: 1100, Avg. Loss: 0.2635077552497387\n",
      "Batch: 1200, Avg. Loss: 0.265502078384161\n",
      "Batch: 1300, Avg. Loss: 0.2653025081753731\n",
      "Batch: 1400, Avg. Loss: 0.2642066363990307\n",
      "Batch: 1500, Avg. Loss: 0.2640555341541767\n",
      "Batch: 1600, Avg. Loss: 0.265454171448946\n",
      "Batch: 1700, Avg. Loss: 0.2643495851755142\n",
      "Batch: 1800, Avg. Loss: 0.26471024975180624\n",
      "Batch: 1900, Avg. Loss: 0.26488485366106035\n",
      "[29.809 secs] Epoch: 2/50, Training loss: 0.2652568199358187\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.2640867176651955\n",
      "Batch: 200, Avg. Loss: 0.2641264034807682\n",
      "Batch: 300, Avg. Loss: 0.26524442225694655\n",
      "Batch: 400, Avg. Loss: 0.26477675437927245\n",
      "Batch: 500, Avg. Loss: 0.26457735627889634\n",
      "Batch: 600, Avg. Loss: 0.26479939237236977\n",
      "Batch: 700, Avg. Loss: 0.2634598256647587\n",
      "Batch: 800, Avg. Loss: 0.2626866614818573\n",
      "Batch: 900, Avg. Loss: 0.2631608407199383\n",
      "Batch: 1000, Avg. Loss: 0.26427771970629693\n",
      "Batch: 1100, Avg. Loss: 0.26390126302838324\n",
      "Batch: 1200, Avg. Loss: 0.2636283400654793\n",
      "Batch: 1300, Avg. Loss: 0.2631534367799759\n",
      "Batch: 1400, Avg. Loss: 0.26393537521362304\n",
      "Batch: 1500, Avg. Loss: 0.2635198180377483\n",
      "Batch: 1600, Avg. Loss: 0.2628684136271477\n",
      "Batch: 1700, Avg. Loss: 0.26373371422290803\n",
      "Batch: 1800, Avg. Loss: 0.2636044418811798\n",
      "Batch: 1900, Avg. Loss: 0.2616813364624977\n",
      "[29.75 secs] Epoch: 3/50, Training loss: 0.2637730649726784\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.2636663997173309\n",
      "Batch: 200, Avg. Loss: 0.26338679328560827\n",
      "Batch: 300, Avg. Loss: 0.26314445048570634\n",
      "Batch: 400, Avg. Loss: 0.260949324965477\n",
      "Batch: 500, Avg. Loss: 0.2641819104552269\n",
      "Batch: 600, Avg. Loss: 0.26412876814603803\n",
      "Batch: 700, Avg. Loss: 0.2625222334265709\n",
      "Batch: 800, Avg. Loss: 0.2643555173277855\n",
      "Batch: 900, Avg. Loss: 0.2628252786397934\n",
      "Batch: 1000, Avg. Loss: 0.2624401547014713\n",
      "Batch: 1100, Avg. Loss: 0.2634214873611927\n",
      "Batch: 1200, Avg. Loss: 0.26524942710995675\n",
      "Batch: 1300, Avg. Loss: 0.2628414624929428\n",
      "Batch: 1400, Avg. Loss: 0.26363385379314425\n",
      "Batch: 1500, Avg. Loss: 0.2635054564476013\n",
      "Batch: 1600, Avg. Loss: 0.2632737927138805\n",
      "Batch: 1700, Avg. Loss: 0.2632192488014698\n",
      "Batch: 1800, Avg. Loss: 0.26182194128632547\n",
      "Batch: 1900, Avg. Loss: 0.26298324286937713\n",
      "[29.853 secs] Epoch: 4/50, Training loss: 0.2632529586019755\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.2621083535254002\n",
      "Batch: 200, Avg. Loss: 0.2632366026937962\n",
      "Batch: 300, Avg. Loss: 0.2631570628285408\n",
      "Batch: 400, Avg. Loss: 0.2611648818850517\n",
      "Batch: 500, Avg. Loss: 0.26412195712327957\n",
      "Batch: 600, Avg. Loss: 0.26408337652683256\n",
      "Batch: 700, Avg. Loss: 0.2630335184931755\n",
      "Batch: 800, Avg. Loss: 0.2631776568293571\n",
      "Batch: 900, Avg. Loss: 0.2626271405816078\n",
      "Batch: 1000, Avg. Loss: 0.2621064944565296\n",
      "Batch: 1100, Avg. Loss: 0.2617578393220901\n",
      "Batch: 1200, Avg. Loss: 0.2626940868794918\n",
      "Batch: 1300, Avg. Loss: 0.26193463668227196\n",
      "Batch: 1400, Avg. Loss: 0.26247707188129427\n",
      "Batch: 1500, Avg. Loss: 0.2632374541461468\n",
      "Batch: 1600, Avg. Loss: 0.2629906871914864\n",
      "Batch: 1700, Avg. Loss: 0.2619922424852848\n",
      "Batch: 1800, Avg. Loss: 0.26428820297122\n",
      "Batch: 1900, Avg. Loss: 0.2623887994885445\n",
      "[30.002 secs] Epoch: 5/50, Training loss: 0.26277483671812096\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.26275493994355203\n",
      "Batch: 200, Avg. Loss: 0.26364617854356764\n",
      "Batch: 300, Avg. Loss: 0.2634231323003769\n",
      "Batch: 400, Avg. Loss: 0.26196006312966347\n",
      "Batch: 500, Avg. Loss: 0.26198949545621875\n",
      "Batch: 600, Avg. Loss: 0.2632572478055954\n",
      "Batch: 700, Avg. Loss: 0.26101676240563393\n",
      "Batch: 800, Avg. Loss: 0.2617567524313927\n",
      "Batch: 900, Avg. Loss: 0.2614071245491505\n",
      "Batch: 1000, Avg. Loss: 0.2615944489836693\n",
      "Batch: 1100, Avg. Loss: 0.2613058489561081\n",
      "Batch: 1200, Avg. Loss: 0.2600044800341129\n",
      "Batch: 1300, Avg. Loss: 0.2609771513938904\n",
      "Batch: 1400, Avg. Loss: 0.2606841704249382\n",
      "Batch: 1500, Avg. Loss: 0.2604824800789356\n",
      "Batch: 1600, Avg. Loss: 0.26002307787537576\n",
      "Batch: 1700, Avg. Loss: 0.2586562614142895\n",
      "Batch: 1800, Avg. Loss: 0.258435560464859\n",
      "Batch: 1900, Avg. Loss: 0.2581774379312992\n",
      "[32.249 secs] Epoch: 6/50, Training loss: 0.26105439358656823\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.2577219420671463\n",
      "Batch: 200, Avg. Loss: 0.25740286499261855\n",
      "Batch: 300, Avg. Loss: 0.2576379656791687\n",
      "Batch: 400, Avg. Loss: 0.2564123313128948\n",
      "Batch: 500, Avg. Loss: 0.25471528857946396\n",
      "Batch: 600, Avg. Loss: 0.25461115062236783\n",
      "Batch: 700, Avg. Loss: 0.25424168676137926\n",
      "Batch: 800, Avg. Loss: 0.2546640242636204\n",
      "Batch: 900, Avg. Loss: 0.2531859564781189\n",
      "Batch: 1000, Avg. Loss: 0.2528844806551933\n",
      "Batch: 1100, Avg. Loss: 0.25305641517043115\n",
      "Batch: 1200, Avg. Loss: 0.25400138288736346\n",
      "Batch: 1300, Avg. Loss: 0.2539651742577553\n",
      "Batch: 1400, Avg. Loss: 0.25145436704158786\n",
      "Batch: 1500, Avg. Loss: 0.2525135238468647\n",
      "Batch: 1600, Avg. Loss: 0.25176056146621706\n",
      "Batch: 1700, Avg. Loss: 0.25106275990605353\n",
      "Batch: 1800, Avg. Loss: 0.2511171980202198\n",
      "Batch: 1900, Avg. Loss: 0.25018880009651184\n",
      "[32.721 secs] Epoch: 7/50, Training loss: 0.25374121967028207\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.24917256638407706\n",
      "Batch: 200, Avg. Loss: 0.24921702429652215\n",
      "Batch: 300, Avg. Loss: 0.24941489681601525\n",
      "Batch: 400, Avg. Loss: 0.24949060261249542\n",
      "Batch: 500, Avg. Loss: 0.24839766979217529\n",
      "Batch: 600, Avg. Loss: 0.24714164942502975\n",
      "Batch: 700, Avg. Loss: 0.24686761066317559\n",
      "Batch: 800, Avg. Loss: 0.24716926708817483\n",
      "Batch: 900, Avg. Loss: 0.24726074442267418\n",
      "Batch: 1000, Avg. Loss: 0.2453451743721962\n",
      "Batch: 1100, Avg. Loss: 0.24822306722402573\n",
      "Batch: 1200, Avg. Loss: 0.24465743452310562\n",
      "Batch: 1300, Avg. Loss: 0.2458776192367077\n",
      "Batch: 1400, Avg. Loss: 0.24438213780522347\n",
      "Batch: 1500, Avg. Loss: 0.2445105469226837\n",
      "Batch: 1600, Avg. Loss: 0.2430090692639351\n",
      "Batch: 1700, Avg. Loss: 0.24489029124379158\n",
      "Batch: 1800, Avg. Loss: 0.24327177450060844\n",
      "Batch: 1900, Avg. Loss: 0.24290288150310516\n",
      "[32.481 secs] Epoch: 8/50, Training loss: 0.2462523973708021\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.24138096168637277\n",
      "Batch: 200, Avg. Loss: 0.24051221266388892\n",
      "Batch: 300, Avg. Loss: 0.24049165427684785\n",
      "Batch: 400, Avg. Loss: 0.2406766141951084\n",
      "Batch: 500, Avg. Loss: 0.24111689776182174\n",
      "Batch: 600, Avg. Loss: 0.23855192720890045\n",
      "Batch: 700, Avg. Loss: 0.24027717858552933\n",
      "Batch: 800, Avg. Loss: 0.23695717513561249\n",
      "Batch: 900, Avg. Loss: 0.2376164935529232\n",
      "Batch: 1000, Avg. Loss: 0.23772450506687165\n",
      "Batch: 1100, Avg. Loss: 0.23856926709413528\n",
      "Batch: 1200, Avg. Loss: 0.23737941324710846\n",
      "Batch: 1300, Avg. Loss: 0.23742083191871644\n",
      "Batch: 1400, Avg. Loss: 0.23717754662036897\n",
      "Batch: 1500, Avg. Loss: 0.23645590320229531\n",
      "Batch: 1600, Avg. Loss: 0.2349258206784725\n",
      "Batch: 1700, Avg. Loss: 0.23487910002470017\n",
      "Batch: 1800, Avg. Loss: 0.23378365978598595\n",
      "Batch: 1900, Avg. Loss: 0.23365373715758322\n",
      "[31.068 secs] Epoch: 9/50, Training loss: 0.23780001304898804\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.23514624044299126\n",
      "Batch: 200, Avg. Loss: 0.23317467510700227\n",
      "Batch: 300, Avg. Loss: 0.23395264595746995\n",
      "Batch: 400, Avg. Loss: 0.232537252753973\n",
      "Batch: 500, Avg. Loss: 0.23366127520799637\n",
      "Batch: 600, Avg. Loss: 0.23390305042266846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 700, Avg. Loss: 0.23444226861000061\n",
      "Batch: 800, Avg. Loss: 0.23341860830783845\n",
      "Batch: 900, Avg. Loss: 0.23287246912717818\n",
      "Batch: 1000, Avg. Loss: 0.23295313939452172\n",
      "Batch: 1100, Avg. Loss: 0.2333099827170372\n",
      "Batch: 1200, Avg. Loss: 0.23226645663380624\n",
      "Batch: 1300, Avg. Loss: 0.23202922984957694\n",
      "Batch: 1400, Avg. Loss: 0.23117508217692376\n",
      "Batch: 1500, Avg. Loss: 0.23244280010461807\n",
      "Batch: 1600, Avg. Loss: 0.23210255041718483\n",
      "Batch: 1700, Avg. Loss: 0.23082134768366813\n",
      "Batch: 1800, Avg. Loss: 0.23259472340345383\n",
      "Batch: 1900, Avg. Loss: 0.23201425164937972\n",
      "[29.801 secs] Epoch: 10/50, Training loss: 0.23284793870109863\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.2318622513115406\n",
      "Batch: 200, Avg. Loss: 0.23106044456362723\n",
      "Batch: 300, Avg. Loss: 0.23092888534069062\n",
      "Batch: 400, Avg. Loss: 0.23104309797286987\n",
      "Batch: 500, Avg. Loss: 0.22986504539847374\n",
      "Batch: 600, Avg. Loss: 0.22964326038956642\n",
      "Batch: 700, Avg. Loss: 0.23021557435393333\n",
      "Batch: 800, Avg. Loss: 0.2304046232998371\n",
      "Batch: 900, Avg. Loss: 0.228971646130085\n",
      "Batch: 1000, Avg. Loss: 0.22879830092191697\n",
      "Batch: 1100, Avg. Loss: 0.22920362040400505\n",
      "Batch: 1200, Avg. Loss: 0.2303116674721241\n",
      "Batch: 1300, Avg. Loss: 0.23009961307048798\n",
      "Batch: 1400, Avg. Loss: 0.23041738897562028\n",
      "Batch: 1500, Avg. Loss: 0.22977389350533486\n",
      "Batch: 1600, Avg. Loss: 0.22869934886693954\n",
      "Batch: 1700, Avg. Loss: 0.22965512722730635\n",
      "Batch: 1800, Avg. Loss: 0.2291492350399494\n",
      "Batch: 1900, Avg. Loss: 0.2293786223232746\n",
      "[31.322 secs] Epoch: 11/50, Training loss: 0.22997807525288974\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.2307782916724682\n",
      "Batch: 200, Avg. Loss: 0.2282691566646099\n",
      "Batch: 300, Avg. Loss: 0.22854001000523566\n",
      "Batch: 400, Avg. Loss: 0.22787577375769616\n",
      "Batch: 500, Avg. Loss: 0.22789188727736473\n",
      "Batch: 600, Avg. Loss: 0.22832317411899566\n",
      "Batch: 700, Avg. Loss: 0.22772394344210625\n",
      "Batch: 800, Avg. Loss: 0.2278981626033783\n",
      "Batch: 900, Avg. Loss: 0.22918543711304665\n",
      "Batch: 1000, Avg. Loss: 0.22725785046815872\n",
      "Batch: 1100, Avg. Loss: 0.22635688677430152\n",
      "Batch: 1200, Avg. Loss: 0.22825082570314406\n",
      "Batch: 1300, Avg. Loss: 0.2266542212665081\n",
      "Batch: 1400, Avg. Loss: 0.22717735588550567\n",
      "Batch: 1500, Avg. Loss: 0.22661472529172896\n",
      "Batch: 1600, Avg. Loss: 0.22682570695877075\n",
      "Batch: 1700, Avg. Loss: 0.22629016488790513\n",
      "Batch: 1800, Avg. Loss: 0.22506982877850532\n",
      "Batch: 1900, Avg. Loss: 0.22644981756806373\n",
      "[30.877 secs] Epoch: 12/50, Training loss: 0.22749571619648773\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.22602155014872552\n",
      "Batch: 200, Avg. Loss: 0.22530466049909592\n",
      "Batch: 300, Avg. Loss: 0.22501639157533646\n",
      "Batch: 400, Avg. Loss: 0.22360971078276634\n",
      "Batch: 500, Avg. Loss: 0.2241443093121052\n",
      "Batch: 600, Avg. Loss: 0.2241150951385498\n",
      "Batch: 700, Avg. Loss: 0.2231045164167881\n",
      "Batch: 800, Avg. Loss: 0.22504024878144263\n",
      "Batch: 900, Avg. Loss: 0.22381842583417894\n",
      "Batch: 1000, Avg. Loss: 0.22439048573374748\n",
      "Batch: 1100, Avg. Loss: 0.22243974059820176\n",
      "Batch: 1200, Avg. Loss: 0.22204920709133147\n",
      "Batch: 1300, Avg. Loss: 0.22262712314724922\n",
      "Batch: 1400, Avg. Loss: 0.22207910045981408\n",
      "Batch: 1500, Avg. Loss: 0.2222226145863533\n",
      "Batch: 1600, Avg. Loss: 0.2232237209379673\n",
      "Batch: 1700, Avg. Loss: 0.2223381908237934\n",
      "Batch: 1800, Avg. Loss: 0.22343804597854613\n",
      "Batch: 1900, Avg. Loss: 0.22156357526779175\n",
      "[29.808 secs] Epoch: 13/50, Training loss: 0.22346845858691292\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.22316538363695146\n",
      "Batch: 200, Avg. Loss: 0.22116582632064818\n",
      "Batch: 300, Avg. Loss: 0.22184632271528243\n",
      "Batch: 400, Avg. Loss: 0.2229314187169075\n",
      "Batch: 500, Avg. Loss: 0.22183918669819833\n",
      "Batch: 600, Avg. Loss: 0.22158435478806496\n",
      "Batch: 700, Avg. Loss: 0.2211594034731388\n",
      "Batch: 800, Avg. Loss: 0.22246329426765443\n",
      "Batch: 900, Avg. Loss: 0.22122686102986336\n",
      "Batch: 1000, Avg. Loss: 0.2214487026631832\n",
      "Batch: 1100, Avg. Loss: 0.22166411325335503\n",
      "Batch: 1200, Avg. Loss: 0.22248648211359978\n",
      "Batch: 1300, Avg. Loss: 0.22139952957630157\n",
      "Batch: 1400, Avg. Loss: 0.22093311533331872\n",
      "Batch: 1500, Avg. Loss: 0.22082487165927886\n",
      "Batch: 1600, Avg. Loss: 0.22153599426150322\n",
      "Batch: 1700, Avg. Loss: 0.2209136100113392\n",
      "Batch: 1800, Avg. Loss: 0.22098030671477317\n",
      "Batch: 1900, Avg. Loss: 0.21964869990944863\n",
      "[30.477 secs] Epoch: 14/50, Training loss: 0.22147643119249275\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.2209922555088997\n",
      "Batch: 200, Avg. Loss: 0.21972819581627845\n",
      "Batch: 300, Avg. Loss: 0.22072943150997162\n",
      "Batch: 400, Avg. Loss: 0.2207433344423771\n",
      "Batch: 500, Avg. Loss: 0.2216218164563179\n",
      "Batch: 600, Avg. Loss: 0.22019423454999923\n",
      "Batch: 700, Avg. Loss: 0.2202850714325905\n",
      "Batch: 800, Avg. Loss: 0.2209336495399475\n",
      "Batch: 900, Avg. Loss: 0.2208012020587921\n",
      "Batch: 1000, Avg. Loss: 0.22092978224158288\n",
      "Batch: 1100, Avg. Loss: 0.22005865067243577\n",
      "Batch: 1200, Avg. Loss: 0.22012509420514106\n",
      "Batch: 1300, Avg. Loss: 0.21932196632027626\n",
      "Batch: 1400, Avg. Loss: 0.22026124268770217\n",
      "Batch: 1500, Avg. Loss: 0.2199216116964817\n",
      "Batch: 1600, Avg. Loss: 0.22039910197257995\n",
      "Batch: 1700, Avg. Loss: 0.2197263717651367\n",
      "Batch: 1800, Avg. Loss: 0.21972667679190636\n",
      "Batch: 1900, Avg. Loss: 0.21998710960149764\n",
      "[30.516 secs] Epoch: 15/50, Training loss: 0.22034504069368085\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.21992015674710275\n",
      "Batch: 200, Avg. Loss: 0.21892692878842354\n",
      "Batch: 300, Avg. Loss: 0.22010062113404275\n",
      "Batch: 400, Avg. Loss: 0.21939336359500886\n",
      "Batch: 500, Avg. Loss: 0.22006667733192445\n",
      "Batch: 600, Avg. Loss: 0.21908783212304114\n",
      "Batch: 700, Avg. Loss: 0.21901376590132712\n",
      "Batch: 800, Avg. Loss: 0.21850237965583802\n",
      "Batch: 900, Avg. Loss: 0.22088537573814393\n",
      "Batch: 1000, Avg. Loss: 0.2190014299750328\n",
      "Batch: 1100, Avg. Loss: 0.21822702437639235\n",
      "Batch: 1200, Avg. Loss: 0.21999720990657806\n",
      "Batch: 1300, Avg. Loss: 0.2202608923614025\n",
      "Batch: 1400, Avg. Loss: 0.22029510676860808\n",
      "Batch: 1500, Avg. Loss: 0.21923279002308846\n",
      "Batch: 1600, Avg. Loss: 0.21854651108384132\n",
      "Batch: 1700, Avg. Loss: 0.21901822343468666\n",
      "Batch: 1800, Avg. Loss: 0.21930362060666084\n",
      "Batch: 1900, Avg. Loss: 0.21860303446650506\n",
      "[29.748 secs] Epoch: 16/50, Training loss: 0.21938700309764889\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.2197653441131115\n",
      "Batch: 200, Avg. Loss: 0.21798526704311372\n",
      "Batch: 300, Avg. Loss: 0.21854983612895013\n",
      "Batch: 400, Avg. Loss: 0.2198879064619541\n",
      "Batch: 500, Avg. Loss: 0.218103659003973\n",
      "Batch: 600, Avg. Loss: 0.21993932694196702\n",
      "Batch: 700, Avg. Loss: 0.21960051387548446\n",
      "Batch: 800, Avg. Loss: 0.21979118019342422\n",
      "Batch: 900, Avg. Loss: 0.21860962480306625\n",
      "Batch: 1000, Avg. Loss: 0.21872999280691147\n",
      "Batch: 1100, Avg. Loss: 0.21704727083444594\n",
      "Batch: 1200, Avg. Loss: 0.2173393340408802\n",
      "Batch: 1300, Avg. Loss: 0.21808463856577873\n",
      "Batch: 1400, Avg. Loss: 0.2183771014213562\n",
      "Batch: 1500, Avg. Loss: 0.21847783282399177\n",
      "Batch: 1600, Avg. Loss: 0.21841927841305733\n",
      "Batch: 1700, Avg. Loss: 0.2174926733970642\n",
      "Batch: 1800, Avg. Loss: 0.21806191846728326\n",
      "Batch: 1900, Avg. Loss: 0.21817647516727448\n",
      "[30.732 secs] Epoch: 17/50, Training loss: 0.21852383718367366\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.21739427059888838\n",
      "Batch: 200, Avg. Loss: 0.21864032208919526\n",
      "Batch: 300, Avg. Loss: 0.2162059059739113\n",
      "Batch: 400, Avg. Loss: 0.21640121191740036\n",
      "Batch: 500, Avg. Loss: 0.21609233185648918\n",
      "Batch: 600, Avg. Loss: 0.21677780225872995\n",
      "Batch: 700, Avg. Loss: 0.21601865977048873\n",
      "Batch: 800, Avg. Loss: 0.21661230102181434\n",
      "Batch: 900, Avg. Loss: 0.2180219006538391\n",
      "Batch: 1000, Avg. Loss: 0.2168211778998375\n",
      "Batch: 1100, Avg. Loss: 0.21460095956921577\n",
      "Batch: 1200, Avg. Loss: 0.21500700667500497\n",
      "Batch: 1300, Avg. Loss: 0.21496514707803727\n",
      "Batch: 1400, Avg. Loss: 0.21598325908184052\n",
      "Batch: 1500, Avg. Loss: 0.21568341925740242\n",
      "Batch: 1600, Avg. Loss: 0.21426306813955306\n",
      "Batch: 1700, Avg. Loss: 0.21420708626508714\n",
      "Batch: 1800, Avg. Loss: 0.214838604927063\n",
      "Batch: 1900, Avg. Loss: 0.21372322916984557\n",
      "[30.536 secs] Epoch: 18/50, Training loss: 0.21592243370374126\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.21498860359191896\n",
      "Batch: 200, Avg. Loss: 0.21305643826723097\n",
      "Batch: 300, Avg. Loss: 0.21287421748042107\n",
      "Batch: 400, Avg. Loss: 0.21315609008073808\n",
      "Batch: 500, Avg. Loss: 0.213340215831995\n",
      "Batch: 600, Avg. Loss: 0.2124667386710644\n",
      "Batch: 700, Avg. Loss: 0.21383289203047753\n",
      "Batch: 800, Avg. Loss: 0.21265166372060776\n",
      "Batch: 900, Avg. Loss: 0.2124081602692604\n",
      "Batch: 1000, Avg. Loss: 0.2108511008322239\n",
      "Batch: 1100, Avg. Loss: 0.2123771822452545\n",
      "Batch: 1200, Avg. Loss: 0.21116874650120734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1300, Avg. Loss: 0.2130633381009102\n",
      "Batch: 1400, Avg. Loss: 0.21273164063692093\n",
      "Batch: 1500, Avg. Loss: 0.21214801251888274\n",
      "Batch: 1600, Avg. Loss: 0.212166695445776\n",
      "Batch: 1700, Avg. Loss: 0.21266652688384055\n",
      "Batch: 1800, Avg. Loss: 0.21164346486330032\n",
      "Batch: 1900, Avg. Loss: 0.2115333589911461\n",
      "[29.953 secs] Epoch: 19/50, Training loss: 0.21252588744722345\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.21065342396497727\n",
      "Batch: 200, Avg. Loss: 0.21108262076973916\n",
      "Batch: 300, Avg. Loss: 0.21012845784425735\n",
      "Batch: 400, Avg. Loss: 0.21041535809636117\n",
      "Batch: 500, Avg. Loss: 0.210799620449543\n",
      "Batch: 600, Avg. Loss: 0.21129563733935355\n",
      "Batch: 700, Avg. Loss: 0.20992181092500686\n",
      "Batch: 800, Avg. Loss: 0.21068478599190713\n",
      "Batch: 900, Avg. Loss: 0.20959248825907706\n",
      "Batch: 1000, Avg. Loss: 0.21068648084998132\n",
      "Batch: 1100, Avg. Loss: 0.20951804041862487\n",
      "Batch: 1200, Avg. Loss: 0.2100693602859974\n",
      "Batch: 1300, Avg. Loss: 0.2087372548878193\n",
      "Batch: 1400, Avg. Loss: 0.2089216688275337\n",
      "Batch: 1500, Avg. Loss: 0.20890990749001503\n",
      "Batch: 1600, Avg. Loss: 0.2089647164940834\n",
      "Batch: 1700, Avg. Loss: 0.2088967117667198\n",
      "Batch: 1800, Avg. Loss: 0.20784956678748132\n",
      "Batch: 1900, Avg. Loss: 0.2075171008706093\n",
      "[29.71 secs] Epoch: 20/50, Training loss: 0.209693418302885\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.20902647733688354\n",
      "Batch: 200, Avg. Loss: 0.2095094719529152\n",
      "Batch: 300, Avg. Loss: 0.2077303086221218\n",
      "Batch: 400, Avg. Loss: 0.20654560580849649\n",
      "Batch: 500, Avg. Loss: 0.20717711746692657\n",
      "Batch: 600, Avg. Loss: 0.20909747675061227\n",
      "Batch: 700, Avg. Loss: 0.20849176049232482\n",
      "Batch: 800, Avg. Loss: 0.20851292341947555\n",
      "Batch: 900, Avg. Loss: 0.20759672209620475\n",
      "Batch: 1000, Avg. Loss: 0.2077190873026848\n",
      "Batch: 1100, Avg. Loss: 0.20774756044149398\n",
      "Batch: 1200, Avg. Loss: 0.20686529219150543\n",
      "Batch: 1300, Avg. Loss: 0.20700286254286765\n",
      "Batch: 1400, Avg. Loss: 0.20803999692201613\n",
      "Batch: 1500, Avg. Loss: 0.2071322226524353\n",
      "Batch: 1600, Avg. Loss: 0.207151125818491\n",
      "Batch: 1700, Avg. Loss: 0.20617590069770814\n",
      "Batch: 1800, Avg. Loss: 0.20773859232664107\n",
      "Batch: 1900, Avg. Loss: 0.20691708967089653\n",
      "[31.163 secs] Epoch: 21/50, Training loss: 0.20774715458602475\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.2073049047589302\n",
      "Batch: 200, Avg. Loss: 0.20630999356508256\n",
      "Batch: 300, Avg. Loss: 0.2069523601233959\n",
      "Batch: 400, Avg. Loss: 0.20631981819868087\n",
      "Batch: 500, Avg. Loss: 0.20700437262654303\n",
      "Batch: 600, Avg. Loss: 0.20712399899959563\n",
      "Batch: 700, Avg. Loss: 0.20745107099413873\n",
      "Batch: 800, Avg. Loss: 0.20686141073703765\n",
      "Batch: 900, Avg. Loss: 0.20568183198571205\n",
      "Batch: 1000, Avg. Loss: 0.20734593480825425\n",
      "Batch: 1100, Avg. Loss: 0.20714771434664725\n",
      "Batch: 1200, Avg. Loss: 0.20611516505479813\n",
      "Batch: 1300, Avg. Loss: 0.20625479355454446\n",
      "Batch: 1400, Avg. Loss: 0.20649076327681543\n",
      "Batch: 1500, Avg. Loss: 0.20657469272613527\n",
      "Batch: 1600, Avg. Loss: 0.20661721244454384\n",
      "Batch: 1700, Avg. Loss: 0.20624483481049538\n",
      "Batch: 1800, Avg. Loss: 0.20585708409547807\n",
      "Batch: 1900, Avg. Loss: 0.20515819817781447\n",
      "[37.049 secs] Epoch: 22/50, Training loss: 0.20656525787488464\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.2060766625404358\n",
      "Batch: 200, Avg. Loss: 0.20591919258236885\n",
      "Batch: 300, Avg. Loss: 0.2042815001308918\n",
      "Batch: 400, Avg. Loss: 0.20488911733031273\n",
      "Batch: 500, Avg. Loss: 0.20566447660326959\n",
      "Batch: 600, Avg. Loss: 0.206730614900589\n",
      "Batch: 700, Avg. Loss: 0.20649446710944175\n",
      "Batch: 800, Avg. Loss: 0.20696796149015426\n",
      "Batch: 900, Avg. Loss: 0.20714618712663652\n",
      "Batch: 1000, Avg. Loss: 0.20564830169081688\n",
      "Batch: 1100, Avg. Loss: 0.2049972888827324\n",
      "Batch: 1200, Avg. Loss: 0.2048078756034374\n",
      "Batch: 1300, Avg. Loss: 0.2057591338455677\n",
      "Batch: 1400, Avg. Loss: 0.20683126762509346\n",
      "Batch: 1500, Avg. Loss: 0.20679942712187768\n",
      "Batch: 1600, Avg. Loss: 0.20542629256844522\n",
      "Batch: 1700, Avg. Loss: 0.20618378549814223\n",
      "Batch: 1800, Avg. Loss: 0.2064209336042404\n",
      "Batch: 1900, Avg. Loss: 0.20501217305660246\n",
      "[36.05 secs] Epoch: 23/50, Training loss: 0.20594799125847332\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.20621087178587913\n",
      "Batch: 200, Avg. Loss: 0.20431746512651444\n",
      "Batch: 300, Avg. Loss: 0.20493167728185654\n",
      "Batch: 400, Avg. Loss: 0.20555970892310144\n",
      "Batch: 500, Avg. Loss: 0.20491864860057832\n",
      "Batch: 600, Avg. Loss: 0.20474814787507056\n",
      "Batch: 700, Avg. Loss: 0.20424998208880424\n",
      "Batch: 800, Avg. Loss: 0.20453856125473976\n",
      "Batch: 900, Avg. Loss: 0.2059011733531952\n",
      "Batch: 1000, Avg. Loss: 0.2061506661772728\n",
      "Batch: 1100, Avg. Loss: 0.2030348590016365\n",
      "Batch: 1200, Avg. Loss: 0.206827083081007\n",
      "Batch: 1300, Avg. Loss: 0.20560712844133378\n",
      "Batch: 1400, Avg. Loss: 0.20660990417003633\n",
      "Batch: 1500, Avg. Loss: 0.20544978797435762\n",
      "Batch: 1600, Avg. Loss: 0.20501283884048463\n",
      "Batch: 1700, Avg. Loss: 0.20456563100218772\n",
      "Batch: 1800, Avg. Loss: 0.20594606757164002\n",
      "Batch: 1900, Avg. Loss: 0.2054118700325489\n",
      "[34.627 secs] Epoch: 24/50, Training loss: 0.20527488487403736\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.20655260533094405\n",
      "Batch: 200, Avg. Loss: 0.2037099704146385\n",
      "Batch: 300, Avg. Loss: 0.20410278663039208\n",
      "Batch: 400, Avg. Loss: 0.20361852988600732\n",
      "Batch: 500, Avg. Loss: 0.20330434396862984\n",
      "Batch: 600, Avg. Loss: 0.20411742702126504\n",
      "Batch: 700, Avg. Loss: 0.20424211710691453\n",
      "Batch: 800, Avg. Loss: 0.20384349748492242\n",
      "Batch: 900, Avg. Loss: 0.20557691276073456\n",
      "Batch: 1000, Avg. Loss: 0.2044868955016136\n",
      "Batch: 1100, Avg. Loss: 0.20530218958854676\n",
      "Batch: 1200, Avg. Loss: 0.20378574267029761\n",
      "Batch: 1300, Avg. Loss: 0.20535366222262383\n",
      "Batch: 1400, Avg. Loss: 0.2041480115056038\n",
      "Batch: 1500, Avg. Loss: 0.2044001379609108\n",
      "Batch: 1600, Avg. Loss: 0.2054776655137539\n",
      "Batch: 1700, Avg. Loss: 0.20410064220428467\n",
      "Batch: 1800, Avg. Loss: 0.20439897283911704\n",
      "Batch: 1900, Avg. Loss: 0.20394908189773558\n",
      "[30.98 secs] Epoch: 25/50, Training loss: 0.20440351169178894\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.2046994125843048\n",
      "Batch: 200, Avg. Loss: 0.20315311700105668\n",
      "Batch: 300, Avg. Loss: 0.2033301778137684\n",
      "Batch: 400, Avg. Loss: 0.2030768336355686\n",
      "Batch: 500, Avg. Loss: 0.20407065913081168\n",
      "Batch: 600, Avg. Loss: 0.20355984479188918\n",
      "Batch: 700, Avg. Loss: 0.20371830508112906\n",
      "Batch: 800, Avg. Loss: 0.20343171268701554\n",
      "Batch: 900, Avg. Loss: 0.20386961296200753\n",
      "Batch: 1000, Avg. Loss: 0.2050417470932007\n",
      "Batch: 1100, Avg. Loss: 0.20320184171199798\n",
      "Batch: 1200, Avg. Loss: 0.20488254562020303\n",
      "Batch: 1300, Avg. Loss: 0.20422718852758406\n",
      "Batch: 1400, Avg. Loss: 0.2036093492805958\n",
      "Batch: 1500, Avg. Loss: 0.20386738881468772\n",
      "Batch: 1600, Avg. Loss: 0.20220241636037828\n",
      "Batch: 1700, Avg. Loss: 0.20436318650841712\n",
      "Batch: 1800, Avg. Loss: 0.203383469581604\n",
      "Batch: 1900, Avg. Loss: 0.20315692633390425\n",
      "[30.156 secs] Epoch: 26/50, Training loss: 0.20376153830953606\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.2050980146229267\n",
      "Batch: 200, Avg. Loss: 0.20278790533542634\n",
      "Batch: 300, Avg. Loss: 0.20220696985721587\n",
      "Batch: 400, Avg. Loss: 0.20278110280632972\n",
      "Batch: 500, Avg. Loss: 0.2030765815079212\n",
      "Batch: 600, Avg. Loss: 0.20389309659600258\n",
      "Batch: 700, Avg. Loss: 0.2032600849866867\n",
      "Batch: 800, Avg. Loss: 0.2027711746096611\n",
      "Batch: 900, Avg. Loss: 0.2047202990949154\n",
      "Batch: 1000, Avg. Loss: 0.20413372173905373\n",
      "Batch: 1100, Avg. Loss: 0.2024909047782421\n",
      "Batch: 1200, Avg. Loss: 0.2042644239962101\n",
      "Batch: 1300, Avg. Loss: 0.20249157637357712\n",
      "Batch: 1400, Avg. Loss: 0.20510943859815597\n",
      "Batch: 1500, Avg. Loss: 0.20282342478632928\n",
      "Batch: 1600, Avg. Loss: 0.2037155406177044\n",
      "Batch: 1700, Avg. Loss: 0.2034097631275654\n",
      "Batch: 1800, Avg. Loss: 0.20196866050362586\n",
      "Batch: 1900, Avg. Loss: 0.20158960685133934\n",
      "[29.781 secs] Epoch: 27/50, Training loss: 0.20325065855634616\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.20336355656385421\n",
      "Batch: 200, Avg. Loss: 0.20317429810762405\n",
      "Batch: 300, Avg. Loss: 0.20269948706030846\n",
      "Batch: 400, Avg. Loss: 0.20219422921538352\n",
      "Batch: 500, Avg. Loss: 0.20404086917638778\n",
      "Batch: 600, Avg. Loss: 0.20200000673532487\n",
      "Batch: 700, Avg. Loss: 0.2022425988316536\n",
      "Batch: 800, Avg. Loss: 0.20255013793706894\n",
      "Batch: 900, Avg. Loss: 0.20232305571436882\n",
      "Batch: 1000, Avg. Loss: 0.2022141245007515\n",
      "Batch: 1100, Avg. Loss: 0.2016779462993145\n",
      "Batch: 1200, Avg. Loss: 0.201774962246418\n",
      "Batch: 1300, Avg. Loss: 0.20224019810557364\n",
      "Batch: 1400, Avg. Loss: 0.20225369960069656\n",
      "Batch: 1500, Avg. Loss: 0.2013101261854172\n",
      "Batch: 1600, Avg. Loss: 0.20167445555329322\n",
      "Batch: 1700, Avg. Loss: 0.20199119806289673\n",
      "Batch: 1800, Avg. Loss: 0.2010868789255619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 1900, Avg. Loss: 0.2016477380692959\n",
      "[30.31 secs] Epoch: 28/50, Training loss: 0.20226466801387383\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.2020454251766205\n",
      "Batch: 200, Avg. Loss: 0.20373900175094606\n",
      "Batch: 300, Avg. Loss: 0.201545400172472\n",
      "Batch: 400, Avg. Loss: 0.20266517832875253\n",
      "Batch: 500, Avg. Loss: 0.20265408098697663\n",
      "Batch: 600, Avg. Loss: 0.20048748433589936\n",
      "Batch: 700, Avg. Loss: 0.20089795023202897\n",
      "Batch: 800, Avg. Loss: 0.20212845981121064\n",
      "Batch: 900, Avg. Loss: 0.20167502924799918\n",
      "Batch: 1000, Avg. Loss: 0.20214170694351197\n",
      "Batch: 1100, Avg. Loss: 0.20244064778089524\n",
      "Batch: 1200, Avg. Loss: 0.2022572386264801\n",
      "Batch: 1300, Avg. Loss: 0.20207226216793062\n",
      "Batch: 1400, Avg. Loss: 0.2021945609152317\n",
      "Batch: 1500, Avg. Loss: 0.20234355136752127\n",
      "Batch: 1600, Avg. Loss: 0.20172506406903268\n",
      "Batch: 1700, Avg. Loss: 0.20276488184928895\n",
      "Batch: 1800, Avg. Loss: 0.20279771000146865\n",
      "Batch: 1900, Avg. Loss: 0.2029288338124752\n",
      "[29.767 secs] Epoch: 29/50, Training loss: 0.2021741970321173\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.20309391736984253\n",
      "Batch: 200, Avg. Loss: 0.2015046864748001\n",
      "Batch: 300, Avg. Loss: 0.20200607180595398\n",
      "Batch: 400, Avg. Loss: 0.20240993440151214\n",
      "Batch: 500, Avg. Loss: 0.20173513546586036\n",
      "Batch: 600, Avg. Loss: 0.20192805364727973\n",
      "Batch: 700, Avg. Loss: 0.2021181981265545\n",
      "Batch: 800, Avg. Loss: 0.2014675259590149\n",
      "Batch: 900, Avg. Loss: 0.20065708622336387\n",
      "Batch: 1000, Avg. Loss: 0.20195077270269393\n",
      "Batch: 1100, Avg. Loss: 0.20193106755614282\n",
      "Batch: 1200, Avg. Loss: 0.20181742772459985\n",
      "Batch: 1300, Avg. Loss: 0.20236964255571366\n",
      "Batch: 1400, Avg. Loss: 0.20116191431879998\n",
      "Batch: 1500, Avg. Loss: 0.2011276063323021\n",
      "Batch: 1600, Avg. Loss: 0.20275973945856093\n",
      "Batch: 1700, Avg. Loss: 0.20172557085752488\n",
      "Batch: 1800, Avg. Loss: 0.2022602552175522\n",
      "Batch: 1900, Avg. Loss: 0.20160463854670524\n",
      "[30.028 secs] Epoch: 30/50, Training loss: 0.20186556286217613\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.20394041314721106\n",
      "Batch: 200, Avg. Loss: 0.20085931554436684\n",
      "Batch: 300, Avg. Loss: 0.20098212718963623\n",
      "Batch: 400, Avg. Loss: 0.20089586928486824\n",
      "Batch: 500, Avg. Loss: 0.2018054187297821\n",
      "Batch: 600, Avg. Loss: 0.20121742382645608\n",
      "Batch: 700, Avg. Loss: 0.20137316927313806\n",
      "Batch: 800, Avg. Loss: 0.2023717811703682\n",
      "Batch: 900, Avg. Loss: 0.2022206901013851\n",
      "Batch: 1000, Avg. Loss: 0.20205412656068802\n",
      "Batch: 1100, Avg. Loss: 0.20147696554660796\n",
      "Batch: 1200, Avg. Loss: 0.20086688548326492\n",
      "Batch: 1300, Avg. Loss: 0.20198271214962005\n",
      "Batch: 1400, Avg. Loss: 0.2014264690876007\n",
      "Batch: 1500, Avg. Loss: 0.20149650320410728\n",
      "Batch: 1600, Avg. Loss: 0.2009944948554039\n",
      "Batch: 1700, Avg. Loss: 0.20187657818198204\n",
      "Batch: 1800, Avg. Loss: 0.201985714584589\n",
      "Batch: 1900, Avg. Loss: 0.20160369604825973\n",
      "[32.945 secs] Epoch: 31/50, Training loss: 0.20162195828242824\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.20162903130054474\n",
      "Batch: 200, Avg. Loss: 0.20150987535715104\n",
      "Batch: 300, Avg. Loss: 0.20249165654182433\n",
      "Batch: 400, Avg. Loss: 0.20135799139738084\n",
      "Batch: 500, Avg. Loss: 0.20224945217370988\n",
      "Batch: 600, Avg. Loss: 0.20069146811962127\n",
      "Batch: 700, Avg. Loss: 0.20210345357656478\n",
      "Batch: 800, Avg. Loss: 0.20132702007889747\n",
      "Batch: 900, Avg. Loss: 0.20108958184719086\n",
      "Batch: 1000, Avg. Loss: 0.20092623740434645\n",
      "Batch: 1100, Avg. Loss: 0.20244968980550765\n",
      "Batch: 1200, Avg. Loss: 0.20019400715827942\n",
      "Batch: 1300, Avg. Loss: 0.20284174442291258\n",
      "Batch: 1400, Avg. Loss: 0.2010782988369465\n",
      "Batch: 1500, Avg. Loss: 0.20222794130444527\n",
      "Batch: 1600, Avg. Loss: 0.2014670979976654\n",
      "Batch: 1700, Avg. Loss: 0.20157040596008302\n",
      "Batch: 1800, Avg. Loss: 0.20201240003108978\n",
      "Batch: 1900, Avg. Loss: 0.201717566549778\n",
      "[32.009 secs] Epoch: 32/50, Training loss: 0.20163327820042382\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.20171961113810538\n",
      "Batch: 200, Avg. Loss: 0.2005769456923008\n",
      "Batch: 300, Avg. Loss: 0.20108643054962158\n",
      "Batch: 400, Avg. Loss: 0.2011342540383339\n",
      "Batch: 500, Avg. Loss: 0.20044321000576018\n",
      "Batch: 600, Avg. Loss: 0.2017213286459446\n",
      "Batch: 700, Avg. Loss: 0.20141416162252426\n",
      "Batch: 800, Avg. Loss: 0.20128333300352097\n",
      "Batch: 900, Avg. Loss: 0.20152078747749327\n",
      "Batch: 1000, Avg. Loss: 0.2008673395216465\n",
      "Batch: 1100, Avg. Loss: 0.2002832868695259\n",
      "Batch: 1200, Avg. Loss: 0.20202457219362258\n",
      "Batch: 1300, Avg. Loss: 0.20176704853773117\n",
      "Batch: 1400, Avg. Loss: 0.20081716284155846\n",
      "Batch: 1500, Avg. Loss: 0.20073435917496682\n",
      "Batch: 1600, Avg. Loss: 0.2012562620639801\n",
      "Batch: 1700, Avg. Loss: 0.20110174968838693\n",
      "Batch: 1800, Avg. Loss: 0.20150103762745858\n",
      "Batch: 1900, Avg. Loss: 0.20159154668450355\n",
      "[33.05 secs] Epoch: 33/50, Training loss: 0.20121888800342105\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.20024948820471763\n",
      "Batch: 200, Avg. Loss: 0.19970753088593482\n",
      "Batch: 300, Avg. Loss: 0.2005163086950779\n",
      "Batch: 400, Avg. Loss: 0.20016375437378883\n",
      "Batch: 500, Avg. Loss: 0.20098556756973265\n",
      "Batch: 600, Avg. Loss: 0.20124756067991256\n",
      "Batch: 700, Avg. Loss: 0.20114823073148727\n",
      "Batch: 800, Avg. Loss: 0.20095714643597604\n",
      "Batch: 900, Avg. Loss: 0.20082833662629127\n",
      "Batch: 1000, Avg. Loss: 0.2010365667939186\n",
      "Batch: 1100, Avg. Loss: 0.19961433634161949\n",
      "Batch: 1200, Avg. Loss: 0.19992519170045853\n",
      "Batch: 1300, Avg. Loss: 0.20149619862437249\n",
      "Batch: 1400, Avg. Loss: 0.20210476100444794\n",
      "Batch: 1500, Avg. Loss: 0.20205399110913277\n",
      "Batch: 1600, Avg. Loss: 0.20146443277597428\n",
      "Batch: 1700, Avg. Loss: 0.20111190527677536\n",
      "Batch: 1800, Avg. Loss: 0.20078099742531777\n",
      "Batch: 1900, Avg. Loss: 0.20156449303030968\n",
      "[33.341 secs] Epoch: 34/50, Training loss: 0.20085452101979798\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.20090748816728593\n",
      "Batch: 200, Avg. Loss: 0.20051707908511163\n",
      "Batch: 300, Avg. Loss: 0.20088323041796685\n",
      "Batch: 400, Avg. Loss: 0.20098891690373422\n",
      "Batch: 500, Avg. Loss: 0.2004649543762207\n",
      "Batch: 600, Avg. Loss: 0.2008016288280487\n",
      "Batch: 700, Avg. Loss: 0.1994545863568783\n",
      "Batch: 800, Avg. Loss: 0.20031255677342416\n",
      "Batch: 900, Avg. Loss: 0.20016594380140304\n",
      "Batch: 1000, Avg. Loss: 0.19952785938978196\n",
      "Batch: 1100, Avg. Loss: 0.20071135044097901\n",
      "Batch: 1200, Avg. Loss: 0.20001479104161263\n",
      "Batch: 1300, Avg. Loss: 0.20158055245876313\n",
      "Batch: 1400, Avg. Loss: 0.19955471098423005\n",
      "Batch: 1500, Avg. Loss: 0.20034905135631562\n",
      "Batch: 1600, Avg. Loss: 0.20068948358297348\n",
      "Batch: 1700, Avg. Loss: 0.20105471074581147\n",
      "Batch: 1800, Avg. Loss: 0.20012138590216635\n",
      "Batch: 1900, Avg. Loss: 0.19945749804377555\n",
      "[32.798 secs] Epoch: 35/50, Training loss: 0.20040213597314996\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.20021922782063484\n",
      "Batch: 200, Avg. Loss: 0.20007386475801467\n",
      "Batch: 300, Avg. Loss: 0.19870681568980217\n",
      "Batch: 400, Avg. Loss: 0.20055623322725297\n",
      "Batch: 500, Avg. Loss: 0.19987757951021196\n",
      "Batch: 600, Avg. Loss: 0.1988685068488121\n",
      "Batch: 700, Avg. Loss: 0.20075867667794228\n",
      "Batch: 800, Avg. Loss: 0.20087529301643373\n",
      "Batch: 900, Avg. Loss: 0.20015895307064058\n",
      "Batch: 1000, Avg. Loss: 0.19983671694993974\n",
      "Batch: 1100, Avg. Loss: 0.20054834827780724\n",
      "Batch: 1200, Avg. Loss: 0.19938545450568199\n",
      "Batch: 1300, Avg. Loss: 0.20060064777731895\n",
      "Batch: 1400, Avg. Loss: 0.2003057259321213\n",
      "Batch: 1500, Avg. Loss: 0.2000438192486763\n",
      "Batch: 1600, Avg. Loss: 0.20152104526758194\n",
      "Batch: 1700, Avg. Loss: 0.19900440752506257\n",
      "Batch: 1800, Avg. Loss: 0.20033473312854766\n",
      "Batch: 1900, Avg. Loss: 0.20037527948617936\n",
      "[33.103 secs] Epoch: 36/50, Training loss: 0.2000929070074214\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.19935569897294045\n",
      "Batch: 200, Avg. Loss: 0.2007618196308613\n",
      "Batch: 300, Avg. Loss: 0.200046806037426\n",
      "Batch: 400, Avg. Loss: 0.20032005935907363\n",
      "Batch: 500, Avg. Loss: 0.20008510753512382\n",
      "Batch: 600, Avg. Loss: 0.20017042860388756\n",
      "Batch: 700, Avg. Loss: 0.19967084065079688\n",
      "Batch: 800, Avg. Loss: 0.1987479344010353\n",
      "Batch: 900, Avg. Loss: 0.201210355758667\n",
      "Batch: 1000, Avg. Loss: 0.2002941520512104\n",
      "Batch: 1100, Avg. Loss: 0.20060422658920288\n",
      "Batch: 1200, Avg. Loss: 0.19946691527962684\n",
      "Batch: 1300, Avg. Loss: 0.20041955173015594\n",
      "Batch: 1400, Avg. Loss: 0.19991286277770995\n",
      "Batch: 1500, Avg. Loss: 0.20007916107773782\n",
      "Batch: 1600, Avg. Loss: 0.20037830054759978\n",
      "Batch: 1700, Avg. Loss: 0.2008080591261387\n",
      "Batch: 1800, Avg. Loss: 0.1997721889615059\n",
      "Batch: 1900, Avg. Loss: 0.2005201391875744\n",
      "[34.113 secs] Epoch: 37/50, Training loss: 0.20013633674676\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.19977782770991326\n",
      "Batch: 200, Avg. Loss: 0.19882182002067567\n",
      "Batch: 300, Avg. Loss: 0.19987955287098885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 400, Avg. Loss: 0.1998651258647442\n",
      "Batch: 500, Avg. Loss: 0.20005365952849388\n",
      "Batch: 600, Avg. Loss: 0.20033441483974457\n",
      "Batch: 700, Avg. Loss: 0.19899181351065637\n",
      "Batch: 800, Avg. Loss: 0.19999795243144036\n",
      "Batch: 900, Avg. Loss: 0.19987766683101654\n",
      "Batch: 1000, Avg. Loss: 0.19968396589159965\n",
      "Batch: 1100, Avg. Loss: 0.20030816808342933\n",
      "Batch: 1200, Avg. Loss: 0.19958114832639695\n",
      "Batch: 1300, Avg. Loss: 0.19895399257540702\n",
      "Batch: 1400, Avg. Loss: 0.20014411702752113\n",
      "Batch: 1500, Avg. Loss: 0.2004253239929676\n",
      "Batch: 1600, Avg. Loss: 0.19847284406423568\n",
      "Batch: 1700, Avg. Loss: 0.20038951843976974\n",
      "Batch: 1800, Avg. Loss: 0.19945936903357506\n",
      "Batch: 1900, Avg. Loss: 0.20033991619944572\n",
      "[32.245 secs] Epoch: 38/50, Training loss: 0.19975382532684197\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.1998999872803688\n",
      "Batch: 200, Avg. Loss: 0.19812084510922431\n",
      "Batch: 300, Avg. Loss: 0.19993143305182456\n",
      "Batch: 400, Avg. Loss: 0.1998563089966774\n",
      "Batch: 500, Avg. Loss: 0.20033294692635537\n",
      "Batch: 600, Avg. Loss: 0.19937312498688697\n",
      "Batch: 700, Avg. Loss: 0.19874149531126023\n",
      "Batch: 800, Avg. Loss: 0.19915685087442397\n",
      "Batch: 900, Avg. Loss: 0.19972991541028023\n",
      "Batch: 1000, Avg. Loss: 0.19834208413958548\n",
      "Batch: 1100, Avg. Loss: 0.1985680902004242\n",
      "Batch: 1200, Avg. Loss: 0.19832780629396438\n",
      "Batch: 1300, Avg. Loss: 0.1980415613949299\n",
      "Batch: 1400, Avg. Loss: 0.19869089424610137\n",
      "Batch: 1500, Avg. Loss: 0.19914371639490128\n",
      "Batch: 1600, Avg. Loss: 0.19970491096377374\n",
      "Batch: 1700, Avg. Loss: 0.199364775121212\n",
      "Batch: 1800, Avg. Loss: 0.19956573858857155\n",
      "Batch: 1900, Avg. Loss: 0.19771606087684632\n",
      "[33.36 secs] Epoch: 39/50, Training loss: 0.19906161224494326\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.19865391224622728\n",
      "Batch: 200, Avg. Loss: 0.198623448908329\n",
      "Batch: 300, Avg. Loss: 0.19883157208561897\n",
      "Batch: 400, Avg. Loss: 0.1986284200847149\n",
      "Batch: 500, Avg. Loss: 0.1991971366107464\n",
      "Batch: 600, Avg. Loss: 0.19841124176979064\n",
      "Batch: 700, Avg. Loss: 0.1973828275501728\n",
      "Batch: 800, Avg. Loss: 0.19784935548901558\n",
      "Batch: 900, Avg. Loss: 0.19886322051286698\n",
      "Batch: 1000, Avg. Loss: 0.1994193512201309\n",
      "Batch: 1100, Avg. Loss: 0.19914274632930756\n",
      "Batch: 1200, Avg. Loss: 0.1990164703130722\n",
      "Batch: 1300, Avg. Loss: 0.19935685083270072\n",
      "Batch: 1400, Avg. Loss: 0.19875690430402757\n",
      "Batch: 1500, Avg. Loss: 0.19929499313235283\n",
      "Batch: 1600, Avg. Loss: 0.19860683634877205\n",
      "Batch: 1700, Avg. Loss: 0.198568480014801\n",
      "Batch: 1800, Avg. Loss: 0.19827079489827157\n",
      "Batch: 1900, Avg. Loss: 0.19940016955137252\n",
      "[30.668 secs] Epoch: 40/50, Training loss: 0.19867066110931375\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.19760282903909684\n",
      "Batch: 200, Avg. Loss: 0.19800806283950806\n",
      "Batch: 300, Avg. Loss: 0.19896472558379175\n",
      "Batch: 400, Avg. Loss: 0.19890716433525085\n",
      "Batch: 500, Avg. Loss: 0.1982388724386692\n",
      "Batch: 600, Avg. Loss: 0.19861010268330573\n",
      "Batch: 700, Avg. Loss: 0.1990024295449257\n",
      "Batch: 800, Avg. Loss: 0.19909567177295684\n",
      "Batch: 900, Avg. Loss: 0.19883738338947296\n",
      "Batch: 1000, Avg. Loss: 0.19928858816623687\n",
      "Batch: 1100, Avg. Loss: 0.19864284068346025\n",
      "Batch: 1200, Avg. Loss: 0.19838457718491553\n",
      "Batch: 1300, Avg. Loss: 0.19934756353497504\n",
      "Batch: 1400, Avg. Loss: 0.19711263477802277\n",
      "Batch: 1500, Avg. Loss: 0.19987175077199937\n",
      "Batch: 1600, Avg. Loss: 0.19797490298748016\n",
      "Batch: 1700, Avg. Loss: 0.1994517222046852\n",
      "Batch: 1800, Avg. Loss: 0.19751699060201644\n",
      "Batch: 1900, Avg. Loss: 0.1981488785147667\n",
      "[30.001 secs] Epoch: 41/50, Training loss: 0.1986218693846324\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.19869380965828895\n",
      "Batch: 200, Avg. Loss: 0.19814601436257362\n",
      "Batch: 300, Avg. Loss: 0.19705744624137878\n",
      "Batch: 400, Avg. Loss: 0.19758427575230597\n",
      "Batch: 500, Avg. Loss: 0.19812149927020073\n",
      "Batch: 600, Avg. Loss: 0.19735050752758979\n",
      "Batch: 700, Avg. Loss: 0.19751812219619752\n",
      "Batch: 800, Avg. Loss: 0.19792039856314658\n",
      "Batch: 900, Avg. Loss: 0.19903338998556136\n",
      "Batch: 1000, Avg. Loss: 0.19747596070170403\n",
      "Batch: 1100, Avg. Loss: 0.1980285032093525\n",
      "Batch: 1200, Avg. Loss: 0.19728520691394805\n",
      "Batch: 1300, Avg. Loss: 0.19721051409840584\n",
      "Batch: 1400, Avg. Loss: 0.19804583951830865\n",
      "Batch: 1500, Avg. Loss: 0.19847701579332352\n",
      "Batch: 1600, Avg. Loss: 0.19750391751527785\n",
      "Batch: 1700, Avg. Loss: 0.1961962439119816\n",
      "Batch: 1800, Avg. Loss: 0.19668882369995117\n",
      "Batch: 1900, Avg. Loss: 0.19783815622329712\n",
      "[30.593 secs] Epoch: 42/50, Training loss: 0.1976998737701496\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.1983799369633198\n",
      "Batch: 200, Avg. Loss: 0.19702690750360488\n",
      "Batch: 300, Avg. Loss: 0.19793172746896745\n",
      "Batch: 400, Avg. Loss: 0.1975567662715912\n",
      "Batch: 500, Avg. Loss: 0.19825960114598273\n",
      "Batch: 600, Avg. Loss: 0.19787469267845154\n",
      "Batch: 700, Avg. Loss: 0.19765966057777404\n",
      "Batch: 800, Avg. Loss: 0.1977254630625248\n",
      "Batch: 900, Avg. Loss: 0.1962532803416252\n",
      "Batch: 1000, Avg. Loss: 0.1965135632455349\n",
      "Batch: 1100, Avg. Loss: 0.19577931195497514\n",
      "Batch: 1200, Avg. Loss: 0.19750693082809448\n",
      "Batch: 1300, Avg. Loss: 0.196157456189394\n",
      "Batch: 1400, Avg. Loss: 0.19716088324785233\n",
      "Batch: 1500, Avg. Loss: 0.19686649411916732\n",
      "Batch: 1600, Avg. Loss: 0.19687842577695847\n",
      "Batch: 1700, Avg. Loss: 0.19645708531141282\n",
      "Batch: 1800, Avg. Loss: 0.1977224111557007\n",
      "Batch: 1900, Avg. Loss: 0.19785907953977586\n",
      "[33.482 secs] Epoch: 43/50, Training loss: 0.19727541675587149\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.19727182179689406\n",
      "Batch: 200, Avg. Loss: 0.19637948870658875\n",
      "Batch: 300, Avg. Loss: 0.19799034804105758\n",
      "Batch: 400, Avg. Loss: 0.19692400768399237\n",
      "Batch: 500, Avg. Loss: 0.19762599647045134\n",
      "Batch: 600, Avg. Loss: 0.196700539290905\n",
      "Batch: 700, Avg. Loss: 0.19765417724847795\n",
      "Batch: 800, Avg. Loss: 0.1961080840229988\n",
      "Batch: 900, Avg. Loss: 0.19694786101579667\n",
      "Batch: 1000, Avg. Loss: 0.19681999057531357\n",
      "Batch: 1100, Avg. Loss: 0.19687091588973998\n",
      "Batch: 1200, Avg. Loss: 0.19627413749694825\n",
      "Batch: 1300, Avg. Loss: 0.19629487112164498\n",
      "Batch: 1400, Avg. Loss: 0.19607814028859138\n",
      "Batch: 1500, Avg. Loss: 0.19659921064972877\n",
      "Batch: 1600, Avg. Loss: 0.19706544533371925\n",
      "Batch: 1700, Avg. Loss: 0.1968987339735031\n",
      "Batch: 1800, Avg. Loss: 0.1982737398147583\n",
      "Batch: 1900, Avg. Loss: 0.19657702565193177\n",
      "[32.303 secs] Epoch: 44/50, Training loss: 0.19693488799256703\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.19709528669714926\n",
      "Batch: 200, Avg. Loss: 0.19762533336877822\n",
      "Batch: 300, Avg. Loss: 0.19592052027583123\n",
      "Batch: 400, Avg. Loss: 0.19591782853007317\n",
      "Batch: 500, Avg. Loss: 0.19627244159579277\n",
      "Batch: 600, Avg. Loss: 0.19589710265398025\n",
      "Batch: 700, Avg. Loss: 0.1963435597717762\n",
      "Batch: 800, Avg. Loss: 0.19560180440545083\n",
      "Batch: 900, Avg. Loss: 0.19702188774943352\n",
      "Batch: 1000, Avg. Loss: 0.19701031744480133\n",
      "Batch: 1100, Avg. Loss: 0.1961423420906067\n",
      "Batch: 1200, Avg. Loss: 0.19813864812254905\n",
      "Batch: 1300, Avg. Loss: 0.19772988736629485\n",
      "Batch: 1400, Avg. Loss: 0.1970302177965641\n",
      "Batch: 1500, Avg. Loss: 0.19903286501765252\n",
      "Batch: 1600, Avg. Loss: 0.197439569234848\n",
      "Batch: 1700, Avg. Loss: 0.19623724207282067\n",
      "Batch: 1800, Avg. Loss: 0.19682920277118682\n",
      "Batch: 1900, Avg. Loss: 0.19629313439130783\n",
      "[32.855 secs] Epoch: 45/50, Training loss: 0.19682249259070217\n",
      "\n",
      "Batch: 100, Avg. Loss: 0.19611287072300912\n",
      "Batch: 200, Avg. Loss: 0.19609464481472969\n",
      "Batch: 300, Avg. Loss: 0.19675667583942413\n",
      "Batch: 400, Avg. Loss: 0.19656914263963698\n",
      "Batch: 500, Avg. Loss: 0.19646244019269943\n",
      "Batch: 600, Avg. Loss: 0.19588404938578605\n",
      "Batch: 700, Avg. Loss: 0.19711556971073152\n",
      "Batch: 800, Avg. Loss: 0.19730365321040153\n",
      "Batch: 900, Avg. Loss: 0.19648179695010184\n",
      "Batch: 1000, Avg. Loss: 0.19683532953262328\n",
      "Batch: 1100, Avg. Loss: 0.19587715327739716\n",
      "Batch: 1200, Avg. Loss: 0.19578078329563142\n",
      "Batch: 1300, Avg. Loss: 0.19703107386827468\n",
      "Batch: 1400, Avg. Loss: 0.19767484053969384\n",
      "Batch: 1500, Avg. Loss: 0.1965510740876198\n",
      "Batch: 1600, Avg. Loss: 0.19509191706776619\n",
      "Batch: 1700, Avg. Loss: 0.19724830925464631\n"
     ]
    }
   ],
   "source": [
    "# Training model\n",
    "train_loss = train(model, train_loader, criterion, optimizer, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decreased-viking",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=go.Scatter(y=train_loss, mode='lines+markers'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-roads",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading model\n",
    "model_path = '.\\\\models\\\\model_20210603175854.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-james",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(path='.\\\\data\\\\training.h5', key='normal', simulation=2)\n",
    "mse = []\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for data in train_dataset:\n",
    "    X = data['X'].to(device)\n",
    "    output = model(X)\n",
    "    mse.append(criterion(output, X).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arbitrary-gasoline",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=go.Scatter(y=mse, mode='lines+markers'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-congress",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_faulty_dataset = Dataset(path='.\\\\data\\\\training.h5', key='faulty', simulation=(1, 5))\n",
    "\n",
    "mse_faulty = []\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "for data in train_faulty_dataset:\n",
    "    X = data['X'].to(device)\n",
    "    output = model(X)\n",
    "    mse_faulty.append(criterion(output, X).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "welcome-hello",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure(data=go.Scatter(y=mse_faulty, mode='lines+markers'))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-machinery",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
